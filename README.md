# YOLO Active Learning with Classification System

A YOLO-based Active Learning system that supports both traditional classification models and image captioning-based classifiers (BLIP, BLIP2, InstructBLIP, VIT-GPT2) for iterative object detection improvement.

## üåü Key Features

- **YOLO-based Active Learning**: Iterative training for performance improvement
- **Dual Classification Methods**:
  - Traditional CNN-based classifiers (DenseNet121)
  - Image captioning classifiers (BLIP, BLIP2, InstructBLIP, VIT-GPT2)
- **Modular Design**: Each component can be used independently
- **Automated Experiments**: Fully automated execution without user intervention
- **Performance Tracking**: Comprehensive metrics and visualization
- **Cycle Timing**: Detailed timing information for each training cycle

## üìã Requirements

### System Requirements
- Python 3.8+
- CUDA-capable GPU (recommended)
- Minimum 8GB RAM

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/yolo-active-learning.git
cd yolo-active-learning

# Install dependencies
pip install -r requirements.txt

# Optional: Install transformers for captioning classifiers
pip install transformers>=4.30.0
```

## üóÇÔ∏è Project Structure

```
project/
‚îú‚îÄ‚îÄ config.py                    # Experiment configuration management
‚îú‚îÄ‚îÄ utils.py                     # Common utility functions
‚îú‚îÄ‚îÄ classifier.py                # Traditional classification model
‚îú‚îÄ‚îÄ captioning_classifier.py     # Image captioning-based classifier
‚îú‚îÄ‚îÄ detector.py                  # Object detection module
‚îú‚îÄ‚îÄ evaluator.py                 # Performance evaluation module
‚îú‚îÄ‚îÄ active_learning.py           # Main Active Learning class
‚îú‚îÄ‚îÄ main.py                      # Experiment execution script
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üìÅ Data Preparation

### Directory Structure
```
your_project/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ yolo/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.pt                 # YOLO model files
‚îÇ   ‚îî‚îÄ‚îÄ classifiers/
‚îÇ       ‚îî‚îÄ‚îÄ *.pth                # Classification model files (optional)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg                # Training images
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ       ‚îî‚îÄ‚îÄ *.txt                # YOLO format labels (optional)
‚îî‚îÄ‚îÄ results/                     # Output directory (auto-created)
```

### Label Format (YOLO)
```
class_id center_x center_y width height
0 0.5 0.5 0.3 0.4
```

## üöÄ Quick Start

### 1. Basic Experiment

Edit `main.py` to configure your experiment:

```python
# Set your data paths
models_dir = "./models/yolo"
classifiers_dir = "./models/classifiers"
image_dir = "./data/images"
label_dir = "./data/labels"
output_dir = "./results"

# Configure basic parameters
conf_threshold = 0.25
max_cycles = 10
gpu_num = 0

# Choose classifier type
use_classifier = False  # Traditional classifier
use_captioning_classifier = True  # Captioning classifier
```

Run the experiment:
```bash
python main.py
```

### 2. Using Captioning Classifier

Configure the captioning classifier in `main.py`:

```python
# Captioning classifier settings
use_captioning_classifier = True
captioning_model_type = "vit-gpt2"  # Options: "blip", "blip2", "instructblip", "vit-gpt2"
target_keywords = ["car", "vehicle", "truck", "bus", "van"]
```

Supported captioning models:
- **BLIP**: Balanced performance, general-purpose
- **BLIP2**: High performance, larger model
- **InstructBLIP**: Instruction-based captioning
- **VIT-GPT2**: Vision Transformer + GPT-2, good natural language generation

### 3. Using Traditional Classifier

```python
# Traditional classifier settings
use_classifier = True
enable_classifier_retraining = False  # Set to True for retraining each cycle

# Classifier training parameters
classifier_epochs = 20
classifier_batch_size = 16
max_samples_per_class = 500
```

## ‚öôÔ∏è Configuration

### Main Parameters

```python
from config import ExperimentConfig

config = ExperimentConfig(
    # Path settings
    models_dir="./models/yolo",
    classifiers_dir="./models/classifiers",
    image_dir="./data/images",
    label_dir="./data/labels",
    output_dir="./results",

    # Hardware settings
    gpu_num=0,

    # Detection parameters
    conf_threshold=0.25,
    iou_threshold=0.5,
    class_conf_threshold=0.5,
    max_cycles=10,

    # Classifier settings
    use_classifier=False,
    enable_classifier_retraining=False,
    use_captioning_classifier=True,
    captioning_model_type="vit-gpt2",
    target_keywords=['car', 'vehicle'],

    # Training settings
    yolo_epochs=50,
    yolo_batch_size=16,
    yolo_patience=10,

    # Seed for reproducibility
    global_seed=42
)
```

## üìä Results and Analysis

### Output Structure
```
results/
‚îú‚îÄ‚îÄ model_name/
‚îÇ   ‚îú‚îÄ‚îÄ cycle_0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detections/              # Detection result images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ labels/                  # Generated labels
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cycle_timing.json        # Cycle timing information
‚îÇ   ‚îú‚îÄ‚îÄ cycle_1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/                # YOLO training results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification_training/ # Classifier training results
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cropped_objects/         # Cropped object images
‚îÇ   ‚îú‚îÄ‚îÄ performance_metrics.csv      # Performance metrics
‚îÇ   ‚îú‚îÄ‚îÄ performance_summary.txt      # Performance summary
‚îÇ   ‚îú‚îÄ‚îÄ cycle_timing_summary.json    # Overall timing summary
‚îÇ   ‚îî‚îÄ‚îÄ cycle_timing_summary.txt     # Human-readable timing
```

### Performance Metrics
- **mAP50**: Mean Average Precision @ IoU 0.5
- **Precision**: Detection precision
- **Recall**: Detection recall
- **F1-Score**: F1 score
- **Detected_Objects**: Number of detected objects
- **Filtered_Objects**: Number of filtered objects by classifier

### Timing Information
Each cycle's timing is recorded in JSON format:
```json
{
  "cycle": 1,
  "total_duration_minutes": 15.5,
  "step_times": {
    "detection": 120.5,
    "classification": 45.2,
    "training": 800.3
  }
}
```

## üîß Advanced Usage

### 1. Skip Cycle 0 (Baseline)

To skip the baseline measurement and start directly from Cycle 1:

```python
skip_cycle_0 = True  # Set in main.py
```

### 2. Custom Classifier

```python
from classifier import ObjectClassifier

# Load pretrained classifier
classifier = ObjectClassifier("path/to/model.pth")

# Classify object
pred_class, confidence = classifier.classify(cropped_image)
```

### 3. Programmatic Usage

```python
from active_learning import YOLOActiveLearning
from config import ExperimentConfig

# Configure experiment
config = ExperimentConfig(
    models_dir="./models/yolo",
    image_dir="./data/images",
    use_captioning_classifier=True,
    captioning_model_type="vit-gpt2",
    target_keywords=["car", "vehicle"]
)

# Run active learning
al = YOLOActiveLearning(
    config=config,
    model_path="./models/yolo/yolov8n.pt",
    classifier_path=None  # Not needed for captioning classifier
)

al.run(skip_cycle_0=False)
```

## üêõ Troubleshooting

### Common Issues

#### 1. GPU Memory Error
```python
# Reduce batch size in main.py
yolo_batch_size = 8
classifier_batch_size = 8
```

#### 2. Classifier Loading Failure
- Ensure model structure matches the weights
- The system automatically attempts structure adjustment

#### 3. No Detections
- Lower the `conf_threshold` (e.g., 0.1)
- Verify the YOLO model is suitable for your dataset

#### 4. Transformers Not Found
```bash
# Install transformers for captioning classifiers
pip install transformers>=4.30.0
```

### Checking Logs
```bash
# View error logs
cat results/model_name/error_logs/error.log

# View experiment logs
cat results/model_name/logs/experiment_log_*.txt
```

## üìà Experiment Design Examples

### 1. Comparing Classification Methods

Run experiments with different classifiers:
```python
# Experiment 1: No classifier (baseline)
use_classifier = False
use_captioning_classifier = False

# Experiment 2: Traditional classifier
use_classifier = True
use_captioning_classifier = False

# Experiment 3: Captioning classifier
use_classifier = False
use_captioning_classifier = True
```

### 2. Testing Different Captioning Models

```python
# Test each captioning model
models = ["blip", "vit-gpt2", "blip2", "instructblip"]
for model_type in models:
    captioning_model_type = model_type
    # Run experiment
```

### 3. Keyword Sensitivity Analysis

```python
# Test different keyword sets
keyword_sets = [
    ["car"],
    ["car", "vehicle"],
    ["car", "vehicle", "truck", "bus", "van"]
]
```

## üìù Citation

If you use this code in your research, please cite:

```bibtex
@misc{yolo-active-learning,
  author = {Your Name},
  title = {YOLO Active Learning with Classification System},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/yourusername/yolo-active-learning}
}
```

## üìÑ License

This project is created for research purposes. Please specify your license.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìß Contact

For questions or issues, please open an issue on GitHub.

## üôè Acknowledgments

- YOLO: [Ultralytics](https://github.com/ultralytics/ultralytics)
- BLIP: [Salesforce](https://github.com/salesforce/BLIP)
- VIT-GPT2: [NLP Connect](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)

## üìö Additional Resources

- [YOLO Documentation](https://docs.ultralytics.com/)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)
- [PyTorch Documentation](https://pytorch.org/docs/)
